{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from mlagents.envs import UnityEnvironment\n",
    "from mlagents.trainers.demo_loader import load_demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(brain_infos):\n",
    "    states, actions = [], []\n",
    "    for idx, brain_info in enumerate(brain_infos):\n",
    "        if idx > len(brain_infos) - 2:\n",
    "            break\n",
    "        current_brain_info = brain_infos[idx]\n",
    "        next_brain_info = brain_infos[idx + 1]\n",
    "        states.append(current_brain_info.vector_observations[0])\n",
    "        actions.append(next_brain_info.previous_vector_actions[0])\n",
    "    return np.array(states), np.array(actions)\n",
    "\n",
    "def load_data(location):\n",
    "    bp, infos, num_steps = load_demonstration(location)\n",
    "    infos = infos[:max_steps+1]\n",
    "    states, actions = make_data(infos)\n",
    "    return bp, states, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 300\n",
    "expert_demo = './demos/ExpertBall.demo'\n",
    "policy_demo = './demos/HeuristicBall.demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_bp, e_states, e_actions = load_data(expert_demo)\n",
    "p_bp, p_states, p_actions = load_data(policy_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Discrimiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    def __init__(self, s_size, a_size, h_size, lr):\n",
    "        self.h_size = h_size\n",
    "        self.make_inputs(s_size, a_size)\n",
    "        self.make_network()\n",
    "        self.make_loss(lr)\n",
    "        \n",
    "    def make_inputs(self, s_size, a_size):\n",
    "        self.state_in_expert = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        self.action_in_expert = tf.placeholder(shape=[None, a_size], dtype=tf.float32)\n",
    "        self.state_in_policy = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n",
    "        self.action_in_policy = tf.placeholder(shape=[None, a_size], dtype=tf.float32)\n",
    "        \n",
    "    def make_discriminator(self, state_in, action_in, reuse):\n",
    "        with tf.variable_scope(\"discriminator\"):\n",
    "            concat_input = tf.concat([state_in, action_in], axis=1)\n",
    "            \n",
    "            hidden_1 = tf.layers.dense(\n",
    "                concat_input, self.h_size, activation=tf.nn.elu,\n",
    "                name=\"d_hidden_1\", reuse=reuse)\n",
    "            \n",
    "            hidden_2 = tf.layers.dense(\n",
    "                hidden_1, self.h_size, activation=tf.nn.elu, \n",
    "                name=\"d_hidden_2\", reuse=reuse)\n",
    "            \n",
    "            d_value = tf.layers.dense(hidden_2, 1, activation=tf.nn.sigmoid, \n",
    "                                name=\"d_value\", reuse=reuse)\n",
    "            return d_value\n",
    "        \n",
    "    def make_loss(self, learning_rate):\n",
    "        self.de = tf.reduce_mean(self.d_expert)\n",
    "        self.dp = tf.reduce_mean(self.d_policy)\n",
    "        self.d_loss = -tf.reduce_mean(tf.log(self.d_expert + 1e-10) + tf.log(1.0 - self.d_policy + 1e-10))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.update_batch = optimizer.minimize(self.d_loss)\n",
    "        \n",
    "    def make_network(self):\n",
    "        self.d_expert = self.make_discriminator(self.state_in_expert, self.action_in_expert, False)\n",
    "        self.d_policy = self.make_discriminator(self.state_in_policy, self.action_in_policy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epoch = 100\n",
    "s_size = e_bp.vector_observation_space_size\n",
    "a_size = e_bp.vector_action_space_size[0]\n",
    "h_size = 64\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "disc = Discriminator(s_size, a_size, h_size, lr)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_ordering(array):\n",
    "    s = np.arange(len(array))\n",
    "    np.random.shuffle(s)\n",
    "    return s\n",
    "\n",
    "def shuffle_buffer(states, actions):\n",
    "    ordering = random_ordering(states)\n",
    "    shuffle_states = states[ordering]\n",
    "    shuffle_actions = actions[ordering]\n",
    "    return shuffle_states, shuffle_actions\n",
    "\n",
    "def get_batch(index, batch_size, a_size, states, actions):\n",
    "    batch_states = states[index*batch_size:(index+1)*batch_size]\n",
    "    batch_actions = actions[index*batch_size:(index+1)*batch_size]\n",
    "    batch_actions = np.reshape(batch_actions, [-1, a_size])\n",
    "    return batch_states, batch_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Expert Reward: 0.554 Policy Reward: 0.621 Model Loss: 1.665\n",
      "Epoch: 1 Expert Reward: 0.544 Policy Reward: 0.578 Model Loss: 1.504\n",
      "Epoch: 2 Expert Reward: 0.536 Policy Reward: 0.532 Model Loss: 1.437\n",
      "Epoch: 3 Expert Reward: 0.529 Policy Reward: 0.488 Model Loss: 1.309\n",
      "Epoch: 4 Expert Reward: 0.524 Policy Reward: 0.448 Model Loss: 1.213\n",
      "Epoch: 5 Expert Reward: 0.522 Policy Reward: 0.412 Model Loss: 1.114\n",
      "Epoch: 6 Expert Reward: 0.523 Policy Reward: 0.380 Model Loss: 1.110\n",
      "Epoch: 7 Expert Reward: 0.525 Policy Reward: 0.359 Model Loss: 1.072\n",
      "Epoch: 8 Expert Reward: 0.529 Policy Reward: 0.338 Model Loss: 1.045\n",
      "Epoch: 9 Expert Reward: 0.534 Policy Reward: 0.318 Model Loss: 0.983\n",
      "Epoch: 10 Expert Reward: 0.540 Policy Reward: 0.305 Model Loss: 1.017\n",
      "Epoch: 11 Expert Reward: 0.545 Policy Reward: 0.290 Model Loss: 0.982\n",
      "Epoch: 12 Expert Reward: 0.550 Policy Reward: 0.281 Model Loss: 0.927\n",
      "Epoch: 13 Expert Reward: 0.558 Policy Reward: 0.270 Model Loss: 0.907\n",
      "Epoch: 14 Expert Reward: 0.564 Policy Reward: 0.264 Model Loss: 0.966\n",
      "Epoch: 15 Expert Reward: 0.571 Policy Reward: 0.252 Model Loss: 0.825\n",
      "Epoch: 16 Expert Reward: 0.579 Policy Reward: 0.243 Model Loss: 0.772\n",
      "Epoch: 17 Expert Reward: 0.586 Policy Reward: 0.240 Model Loss: 0.821\n",
      "Epoch: 18 Expert Reward: 0.597 Policy Reward: 0.231 Model Loss: 0.883\n",
      "Epoch: 19 Expert Reward: 0.600 Policy Reward: 0.231 Model Loss: 1.023\n",
      "Epoch: 20 Expert Reward: 0.607 Policy Reward: 0.224 Model Loss: 0.849\n",
      "Epoch: 21 Expert Reward: 0.615 Policy Reward: 0.217 Model Loss: 0.835\n",
      "Epoch: 22 Expert Reward: 0.621 Policy Reward: 0.215 Model Loss: 0.986\n",
      "Epoch: 23 Expert Reward: 0.631 Policy Reward: 0.213 Model Loss: 0.681\n",
      "Epoch: 24 Expert Reward: 0.633 Policy Reward: 0.209 Model Loss: 0.776\n",
      "Epoch: 25 Expert Reward: 0.640 Policy Reward: 0.206 Model Loss: 0.845\n",
      "Epoch: 26 Expert Reward: 0.649 Policy Reward: 0.204 Model Loss: 0.685\n",
      "Epoch: 27 Expert Reward: 0.653 Policy Reward: 0.201 Model Loss: 0.632\n",
      "Epoch: 28 Expert Reward: 0.659 Policy Reward: 0.195 Model Loss: 0.728\n",
      "Epoch: 29 Expert Reward: 0.665 Policy Reward: 0.194 Model Loss: 0.727\n",
      "Epoch: 30 Expert Reward: 0.670 Policy Reward: 0.192 Model Loss: 0.704\n",
      "Epoch: 31 Expert Reward: 0.677 Policy Reward: 0.189 Model Loss: 0.681\n",
      "Epoch: 32 Expert Reward: 0.684 Policy Reward: 0.189 Model Loss: 0.591\n",
      "Epoch: 33 Expert Reward: 0.689 Policy Reward: 0.184 Model Loss: 0.662\n",
      "Epoch: 34 Expert Reward: 0.695 Policy Reward: 0.181 Model Loss: 0.684\n",
      "Epoch: 35 Expert Reward: 0.698 Policy Reward: 0.179 Model Loss: 0.585\n",
      "Epoch: 36 Expert Reward: 0.702 Policy Reward: 0.179 Model Loss: 0.545\n",
      "Epoch: 37 Expert Reward: 0.707 Policy Reward: 0.180 Model Loss: 0.685\n",
      "Epoch: 38 Expert Reward: 0.712 Policy Reward: 0.173 Model Loss: 0.599\n",
      "Epoch: 39 Expert Reward: 0.716 Policy Reward: 0.172 Model Loss: 0.652\n",
      "Epoch: 40 Expert Reward: 0.721 Policy Reward: 0.170 Model Loss: 0.560\n",
      "Epoch: 41 Expert Reward: 0.726 Policy Reward: 0.167 Model Loss: 0.414\n",
      "Epoch: 42 Expert Reward: 0.730 Policy Reward: 0.163 Model Loss: 0.720\n",
      "Epoch: 43 Expert Reward: 0.737 Policy Reward: 0.166 Model Loss: 0.512\n",
      "Epoch: 44 Expert Reward: 0.738 Policy Reward: 0.165 Model Loss: 0.466\n",
      "Epoch: 45 Expert Reward: 0.742 Policy Reward: 0.155 Model Loss: 0.692\n",
      "Epoch: 46 Expert Reward: 0.748 Policy Reward: 0.158 Model Loss: 0.487\n",
      "Epoch: 47 Expert Reward: 0.752 Policy Reward: 0.159 Model Loss: 0.441\n",
      "Epoch: 48 Expert Reward: 0.757 Policy Reward: 0.152 Model Loss: 0.529\n",
      "Epoch: 49 Expert Reward: 0.760 Policy Reward: 0.158 Model Loss: 0.601\n",
      "Epoch: 50 Expert Reward: 0.767 Policy Reward: 0.155 Model Loss: 0.535\n",
      "Epoch: 51 Expert Reward: 0.772 Policy Reward: 0.155 Model Loss: 0.639\n",
      "Epoch: 52 Expert Reward: 0.770 Policy Reward: 0.153 Model Loss: 0.462\n",
      "Epoch: 53 Expert Reward: 0.773 Policy Reward: 0.149 Model Loss: 0.432\n",
      "Epoch: 54 Expert Reward: 0.775 Policy Reward: 0.145 Model Loss: 0.426\n",
      "Epoch: 55 Expert Reward: 0.777 Policy Reward: 0.142 Model Loss: 0.379\n",
      "Epoch: 56 Expert Reward: 0.782 Policy Reward: 0.149 Model Loss: 0.394\n",
      "Epoch: 57 Expert Reward: 0.785 Policy Reward: 0.147 Model Loss: 0.388\n",
      "Epoch: 58 Expert Reward: 0.790 Policy Reward: 0.139 Model Loss: 0.451\n",
      "Epoch: 59 Expert Reward: 0.791 Policy Reward: 0.143 Model Loss: 0.624\n",
      "Epoch: 60 Expert Reward: 0.794 Policy Reward: 0.143 Model Loss: 0.467\n",
      "Epoch: 61 Expert Reward: 0.796 Policy Reward: 0.139 Model Loss: 0.757\n",
      "Epoch: 62 Expert Reward: 0.801 Policy Reward: 0.143 Model Loss: 0.608\n",
      "Epoch: 63 Expert Reward: 0.803 Policy Reward: 0.138 Model Loss: 0.512\n",
      "Epoch: 64 Expert Reward: 0.806 Policy Reward: 0.136 Model Loss: 0.560\n",
      "Epoch: 65 Expert Reward: 0.812 Policy Reward: 0.135 Model Loss: 0.379\n",
      "Epoch: 66 Expert Reward: 0.814 Policy Reward: 0.134 Model Loss: 0.523\n",
      "Epoch: 67 Expert Reward: 0.815 Policy Reward: 0.135 Model Loss: 0.473\n",
      "Epoch: 68 Expert Reward: 0.816 Policy Reward: 0.133 Model Loss: 0.336\n",
      "Epoch: 69 Expert Reward: 0.818 Policy Reward: 0.132 Model Loss: 0.420\n",
      "Epoch: 70 Expert Reward: 0.823 Policy Reward: 0.129 Model Loss: 0.287\n",
      "Epoch: 71 Expert Reward: 0.826 Policy Reward: 0.120 Model Loss: 0.299\n",
      "Epoch: 72 Expert Reward: 0.827 Policy Reward: 0.126 Model Loss: 0.361\n",
      "Epoch: 73 Expert Reward: 0.827 Policy Reward: 0.127 Model Loss: 0.375\n",
      "Epoch: 74 Expert Reward: 0.831 Policy Reward: 0.128 Model Loss: 0.413\n",
      "Epoch: 75 Expert Reward: 0.835 Policy Reward: 0.130 Model Loss: 0.291\n",
      "Epoch: 76 Expert Reward: 0.833 Policy Reward: 0.124 Model Loss: 0.254\n",
      "Epoch: 77 Expert Reward: 0.835 Policy Reward: 0.119 Model Loss: 0.288\n",
      "Epoch: 78 Expert Reward: 0.842 Policy Reward: 0.126 Model Loss: 0.286\n",
      "Epoch: 79 Expert Reward: 0.840 Policy Reward: 0.122 Model Loss: 0.502\n",
      "Epoch: 80 Expert Reward: 0.842 Policy Reward: 0.126 Model Loss: 0.365\n",
      "Epoch: 81 Expert Reward: 0.843 Policy Reward: 0.123 Model Loss: 0.279\n",
      "Epoch: 82 Expert Reward: 0.845 Policy Reward: 0.122 Model Loss: 0.280\n",
      "Epoch: 83 Expert Reward: 0.846 Policy Reward: 0.118 Model Loss: 0.443\n",
      "Epoch: 84 Expert Reward: 0.849 Policy Reward: 0.120 Model Loss: 0.524\n",
      "Epoch: 85 Expert Reward: 0.850 Policy Reward: 0.118 Model Loss: 0.349\n",
      "Epoch: 86 Expert Reward: 0.852 Policy Reward: 0.117 Model Loss: 0.219\n",
      "Epoch: 87 Expert Reward: 0.856 Policy Reward: 0.118 Model Loss: 0.469\n",
      "Epoch: 88 Expert Reward: 0.854 Policy Reward: 0.119 Model Loss: 0.196\n",
      "Epoch: 89 Expert Reward: 0.858 Policy Reward: 0.117 Model Loss: 0.271\n",
      "Epoch: 90 Expert Reward: 0.861 Policy Reward: 0.113 Model Loss: 0.345\n",
      "Epoch: 91 Expert Reward: 0.859 Policy Reward: 0.111 Model Loss: 0.290\n",
      "Epoch: 92 Expert Reward: 0.860 Policy Reward: 0.113 Model Loss: 0.236\n",
      "Epoch: 93 Expert Reward: 0.861 Policy Reward: 0.112 Model Loss: 0.577\n",
      "Epoch: 94 Expert Reward: 0.863 Policy Reward: 0.111 Model Loss: 0.596\n",
      "Epoch: 95 Expert Reward: 0.864 Policy Reward: 0.112 Model Loss: 0.345\n",
      "Epoch: 96 Expert Reward: 0.865 Policy Reward: 0.102 Model Loss: 0.335\n",
      "Epoch: 97 Expert Reward: 0.867 Policy Reward: 0.107 Model Loss: 0.296\n",
      "Epoch: 98 Expert Reward: 0.871 Policy Reward: 0.103 Model Loss: 0.316\n",
      "Epoch: 99 Expert Reward: 0.873 Policy Reward: 0.107 Model Loss: 0.345\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epoch):\n",
    "    e_states, e_actions = shuffle_buffer(e_states, e_actions)    \n",
    "    p_states, p_actions = shuffle_buffer(p_states, p_actions)\n",
    "    p_batch_reward, e_batch_reward = [], []\n",
    "\n",
    "    for j in range(len(p_states)//batch_size):\n",
    "        e_batch_states, e_batch_actions = get_batch(j, batch_size, a_size, e_states, e_actions)\n",
    "        p_batch_states, p_batch_actions = get_batch(j, batch_size, a_size, p_states, p_actions)\n",
    "        \n",
    "        feed_dict = {disc.state_in_expert: e_batch_states, \n",
    "                     disc.state_in_policy: p_batch_states,\n",
    "                     disc.action_in_expert: e_batch_actions, \n",
    "                     disc.action_in_policy: p_batch_actions}\n",
    "        \n",
    "        run_list = [disc.de, disc.dp, disc.d_loss, disc.update_batch]\n",
    "        d_e, d_p, loss, _ = sess.run(run_list, feed_dict=feed_dict)\n",
    "        e_batch_reward.append(d_e)\n",
    "        p_batch_reward.append(d_p)\n",
    "    print(\"Epoch: {}\".format(i), \n",
    "          \"Expert Reward: {:.3f}\".format(np.mean(e_batch_reward)), \n",
    "          \"Policy Reward: {:.3f}\".format(np.mean(p_batch_reward)), \n",
    "          \"Model Loss: {:.3f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
